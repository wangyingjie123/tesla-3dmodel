<template>
  <div class="webrtc">
    <div class="webrtc-video margin-right-20">
      <video ref="video" class="webrtc-video__inner" width="480" height="300" autoplay playsinline muted></video>
      <el-button type="primary" @click="takePhoto" class="button">æ‹æ‘„</el-button>
      <el-button @click="setVideoBackground">è®¾ç½®è™šæ‹ŸèƒŒæ™¯</el-button>
    </div>
    <div class="webrtc-img">
      <img
        v-for="(item, index) in imgList"
        :key="index"
        :src="item"
        alt=""
        class="webrtc-img__item margin-right-10 margin-bottom-10"
      />
    </div>
    <div class="webrtc-device margin-left-20">
      <div class="webrtc-device__list">
        <label class="webrtc-device__label">åˆ‡æ¢è®¾å¤‡</label>
        <select>
          <option v-for="item in deviceList" :key="item.deviceId" :value="item.deviceId">{{ item.label }}</option>
        </select>
      </div>
      <div class="webrtc-device__list">
        <label class="webrtc-device__label">åˆ‡æ¢æ–¹å‘</label>
        <select v-model="cameraDirection" @change="switchCamera">
          <option :value="1">å‰ç½®æ‘„åƒå¤´</option>
          <option :value="2">åç½®æ‘„åƒå¤´</option>
        </select>
      </div>
      <Shared @playStream="playLocalStream"></Shared>
    </div>
  </div>
  <Video ref="sonRef"></Video>
</template>
<script lang="ts" setup>
import { ref, onMounted, onUnmounted } from 'vue';
import Shared from './shared.vue';
import Video from './video.vue';
const video = ref<HTMLVideoElement>();
const imgList = ref<string[]>([]);
const sonRef = ref<{ onMounted: (video: HTMLVideoElement) => void }>();
const deviceList = ref<MediaDeviceInfo[]>([]);
const cameraDirection = ref(1);

let localStream: MediaStream;
// è·å–æœ¬åœ°éŸ³è§†é¢‘æµ
async function getLocalStream(constraints?: MediaStreamConstraints) {
  // è·å–åª’ä½“æµ
  localStream = await navigator.mediaDevices.getUserMedia(constraints);
  playLocalStream(localStream);
}

// æ’­æ”¾æœ¬åœ°è§†é¢‘æµ
function playLocalStream(stream: MediaStream) {
  video.value!.srcObject = stream;
}
// æ‹ç…§
function takePhoto() {
  const videoEl = video.value as HTMLVideoElement;
  const canvas = document.createElement('canvas');
  canvas.width = videoEl.videoWidth;
  canvas.height = videoEl.videoHeight;
  const ctx = canvas.getContext('2d')!;
  ctx.drawImage(videoEl, 0, 0, canvas.width, canvas.height);
  imgList.value.push(canvas.toDataURL('image/png'));

  // æ·»åŠ æ»¤é•œ
  const filterList = [
    'blur(5px)', // æ¨¡ç³Š
    'brightness(0.5)', // äº®åº¦
    'contrast(200%)', // å¯¹æ¯”åº¦
    'grayscale(100%)', // ç°åº¦
    'hue-rotate(90deg)', // è‰²ç›¸æ—‹è½¬
    'invert(100%)', // åè‰²
    'opacity(90%)', // é€æ˜åº¦
    'saturate(200%)', // é¥±å’Œåº¦
    'saturate(20%)', // é¥±å’Œåº¦
    'sepia(100%)', // è¤è‰²
    'drop-shadow(4px 4px 8px blue)', // é˜´å½±
  ];
  imgList.value = [];
  for (let i = 0; i < filterList.length; i++) {
    ctx.filter = filterList[i];
    ctx.drawImage(videoEl, 0, 0, canvas.width, canvas.height);
    imgList.value.push(canvas.toDataURL('image/png'));
  }
}

// åˆ‡æ¢æ‘„åƒå¤´
const switchCamera = () => {
  const constraints: MediaStreamConstraints = {
    video: true, // å¼€å¯é»˜è®¤æ‘„åƒå¤´
    audio: true,
  };
  constraints.video = {
    // å¼ºåˆ¶åˆ‡æ¢å‰åæ‘„åƒå¤´æ—¶ï¼Œå½“æ‘„åƒå¤´ä¸æ”¯æŒæ—¶ï¼Œä¼šæŠ¥ä¸€ä¸ªOverconstrainedErrorï¼»æ— æ³•æ»¡è¶³è¦æ±‚çš„é”™è¯¯ï¼½
    facingMode: { exact: cameraDirection.value === 1 ? 'user' : 'environment' },
    // ä¹Ÿå¯ä»¥è¿™æ ·å½“å‰åæ‘„åƒå¤´ä¸æ”¯æŒåˆ‡æ¢æ—¶ï¼Œä¼šç»§ç»­ä½¿ç”¨å½“å‰æ‘„åƒå¤´ï¼Œå¥½å¤„æ˜¯ä¸ä¼šæŠ¥é”™
    // facingMode: val === 1 ? 'user' : 'environment',
  };
  navigator.mediaDevices
    .getUserMedia(constraints)
    .then((stream) => {
      console.log('åˆ‡æ¢æˆåŠŸ');
      playLocalStream(stream);
    })
    .catch((err) => {
      console.error(`ä½ çš„è®¾å¤‡ä¸æ”¯æŒåˆ‡æ¢å‰åæ‘„åƒå¤´ ${err}`);
    });
};
// è·å–æ‰€æœ‰è§†é¢‘è¾“å…¥è®¾å¤‡
async function getDevices() {
  const devices = await navigator.mediaDevices.enumerateDevices();
  console.log('ğŸš€ğŸš€ğŸš€ / è®¾å¤‡åˆ—è¡¨', devices);
  const videoDevices = devices.filter((device) => device.kind === 'videoinput');
  deviceList.value = videoDevices;
  console.log('ğŸš€ğŸš€ğŸš€ / æ‘„åƒå¤´åˆ—è¡¨', videoDevices);
}
// è™šæ‹ŸèƒŒæ™¯
const setVideoBackground = () => {
  sonRef.value?.onMounted(video.value as HTMLVideoElement);
};
onMounted(() => {
  getLocalStream({
    video: true,
    audio: true,
  });
  getDevices();
});

onUnmounted(() => {
  localStream.getVideoTracks().forEach((track) => {
    track.enabled = false;
  });
});
</script>
<style scoped lang="scss">
.webrtc {
  width: 100%;
  background-color: #fff;
  display: flex;
  &-video {
    width: 480px;
    text-align: center;
    &__inner {
      width: 100%;
    }
  }
  &-img {
    display: flex;
    width: 50%;
    flex-wrap: wrap;
    &__item {
      width: 120px;
    }
  }
  &-device {
    &__list {
      line-height: 40px;
      display: flex;
      align-items: center;
      gap: 10px;
    }
  }
}
</style>
